{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "Copyright 2020 Hamaad Musharaf Shah\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic feature engineering using Generative Adversarial Networks\n",
    "## Author: Hamaad Shah\n",
    "\n",
    "---\n",
    "\n",
    "The purpose of deep learning is to learn a representation of high dimensional and noisy data using a sequence of differentiable functions, i.e., geometric transformations, that can perhaps be used for supervised learning tasks among others. It has had great success in discriminative models while generative models have fared worse due to the limitations of explicit maximum likelihood estimation (MLE). Adversarial learning as presented in the Generative Adversarial Network (GAN) aims to overcome these problems by using implicit MLE. \n",
    "\n",
    "We will use the MNIST computer vision dataset for these experiments. GAN is a remarkably different method of learning compared to explicit MLE. Our purpose will be to show that the representation learnt by a GAN in an unsupervised manner can be used for supervised learning tasks. Unlabelled data is inexpensive to obtain in large quantities therefore training a feature extractor in an unsupervised manner is a powerful first step towards later training supervised learning models which perhaps may not have access to a large amount of labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices(device_type=\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(device=gpu_devices[0], enable=True)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import plotnine\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU:\", gpu_devices)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = (\n",
    "    x_train.astype(dtype=\"float32\") / 255.0,\n",
    "    x_test.astype(dtype=\"float32\") / 255.0,\n",
    ")\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "class_per_label_size = 100\n",
    "sampled_class_ids = np.concatenate(\n",
    "    [\n",
    "        [\n",
    "            np.random.choice(\n",
    "                a=np.arange(start=0, stop=y_train.shape[0], step=1)[\n",
    "                    y_train == class_label\n",
    "                ],\n",
    "                size=class_per_label_size,\n",
    "                replace=False,\n",
    "            )\n",
    "        ]\n",
    "        for class_label in np.unique(ar=y_train)\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "sampled_x_train = np.concatenate(\n",
    "    [x_train[these_class_ids, :, :] for these_class_ids in sampled_class_ids], axis=0\n",
    ")\n",
    "sampled_y_train = np.concatenate(\n",
    "    [y_train[these_class_ids] for these_class_ids in sampled_class_ids], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network\n",
    "\n",
    "---\n",
    "\n",
    "There are 2 main components to a GAN, the generator and the discriminator, that play an adversarial game against each other. In doing so the generator learns how to create realistic synthetic samples from noise, i.e., the latent space $z$, while the discriminator learns how to distinguish between a real sample and a synthetic sample. \n",
    "\n",
    "The representation learnt by the discriminator can later on be used for other supervised learning tasks, i.e., automatic feature engineering or representation learning. This can also be viewed through the lens of transfer learning. A GAN can also be used for semi-supervised learning which we will get to in another paper where we will look into using variational autoencoders, ladder networks and adversarial autoencoders for this purpose.\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "We will use the MNIST dataset for this purpose where the raw data is a 2 dimensional tensor of pixel intensities per image. The image is our unit of analysis: We will predict the probability of each class for each image. This is a multiclass classification task and we will use the accuracy score to assess model performance on the test fold.\n",
    "\n",
    "![](pixel_lattice.png)\n",
    "\n",
    "Some examples of handcrafted feature engineering for the computer vision task perhaps might be using Gabor filters.\n",
    "\n",
    "### Generator\n",
    "\n",
    "---\n",
    "\n",
    "Assume that we have a prior belief on where the latent space $z$ lies: $p(z)$. Given a draw from this latent space the generator $G$, a deep learner parameterized by $\\theta_{G}$, outputs a synthetic sample.\n",
    "\n",
    "$$\n",
    "G(z|\\theta_{G}): z \\rightarrow x_{synthetic}\n",
    "$$ \n",
    "\n",
    "### Discriminator\n",
    "\n",
    "---\n",
    "\n",
    "The discriminator $D$ is another deep learner parameterized by $\\theta_{D}$ and it aims to classify if a sample is real or synthetic, i.e., if a sample is from the real data distribution,\n",
    "\n",
    "$$\n",
    "P_{\\text{data}}\n",
    "$$ \n",
    "\n",
    "or the synthetic data distribution.\n",
    "\n",
    "$$\n",
    "P_{G}\n",
    "$$\n",
    "\n",
    "Let us denote the discriminator $D$ as follows.\n",
    "\n",
    "$$\n",
    "D(x|\\theta_{D}): x \\rightarrow [0, 1]\n",
    "$$ \n",
    "\n",
    "Here we assume that the positive examples are from the real data distribution while the negative examples are from the synthetic data distribution.\n",
    "\n",
    "### Game: Optimality\n",
    "\n",
    "---\n",
    "\n",
    "A GAN simultaneously trains the discriminator to correctly classify real and synthetic examples while training the generator to create synthetic examples such that the discriminator incorrectly classifies real and synthetic examples. This 2 player minimax game has the following objective function.\n",
    "\n",
    "$$\n",
    "\\min_{G(z|\\theta_{G})} \\max_{D(x|\\theta_{D})} V(D(x|\\theta_{D}), G(z|\\theta_{G})) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{D(x|\\theta_{D})} + \\mathbb{E}_{z \\sim p(z)} \\log{(1 - D(G(z|\\theta_{G})|\\theta_{D}))}\n",
    "$$\n",
    "\n",
    "Please note that the above expression is basically the objective function of the discriminator.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{D(x|\\theta_{D})} + \\mathbb{E}_{x \\sim p_{G}(x)} \\log{(1 - D(x|\\theta_{D}))}\n",
    "$$\n",
    "\n",
    "It is clear from how the game has been set up that we are trying to obtain a solution $\\theta_{D}$ for $D$ such that it maximizes $V(D, G)$ while simultaneously we are trying to obtain a solution $\\theta_{G}$ for $G$ such that it minimizes $V(D, G)$.\n",
    "\n",
    "We do not simultaneously train $D$ and $G$. We train them alternately: Train $D$ and then train $G$ while freezing $D$. We repeat this for a fixed number of steps.\n",
    "\n",
    "If the synthetic samples taken from the generator $G$ are realistic then implicitly we have learnt the distribution $P_{G}$. In other words, $P_{G}$ can be seen as a good estimation of $P_{\\text{data}}$. The optimal solution will be as follows.\n",
    "\n",
    "$$\n",
    "P_{G}=P_{\\text{data}}\n",
    "$$\n",
    "\n",
    "To show this let us find the optimal discriminator $D^\\ast$ given a generator $G$ and sample $x$. \n",
    "\n",
    "\\begin{align*}\n",
    "V(D, G) &= \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{D(x|\\theta_{D})} + \\mathbb{E}_{x \\sim p_{G}(x)} \\log{(1 - D(x|\\theta_{D}))} \\\\\n",
    "&= \\int_{x} p_{\\text{data}}(x) \\log{D(x|\\theta_{D})} dx + \\int_{x} p_{G}(x) \\log{(1 - D(x|\\theta_{D}))} dx \\\\\n",
    "&= \\int_{x} \\underbrace{p_{\\text{data}}(x) \\log{D(x|\\theta_{D})} + p_{G}(x) \\log{(1 - D(x|\\theta_{D}))}}_{J(D(x|\\theta_{D}))} dx\n",
    "\\end{align*}\n",
    "\n",
    "Let us take a closer look at the discriminator's objective function for a sample $x$.\n",
    "\n",
    "\\begin{align*}\n",
    "J(D(x|\\theta_{D})) &= p_{\\text{data}}(x) \\log{D(x|\\theta_{D})} + p_{G}(x) \\log{(1 - D(x|\\theta_{D}))} \\\\\n",
    "\\frac{\\partial J(D(x|\\theta_{D}))}{\\partial D(x|\\theta_{D})} &= \\frac{p_{\\text{data}}(x)}{D(x|\\theta_{D})} - \\frac{p_{G}(x)}{(1 - D(x|\\theta_{D}))} \\\\\n",
    "0 &= \\frac{p_{\\text{data}}(x)}{D^\\ast(x|\\theta_{D^\\ast})} - \\frac{p_{G}(x)}{(1 - D^\\ast(x|\\theta_{D^\\ast}))} \\\\\n",
    "p_{\\text{data}}(x)(1 - D^\\ast(x|\\theta_{D^\\ast})) &= p_{G}(x)D^\\ast(x|\\theta_{D^\\ast}) \\\\\n",
    "p_{\\text{data}}(x) - p_{\\text{data}}(x)D^\\ast(x|\\theta_{D^\\ast})) &= p_{G}(x)D^\\ast(x|\\theta_{D^\\ast}) \\\\\n",
    "p_{G}(x)D^\\ast(x|\\theta_{D^\\ast}) + p_{\\text{data}}(x)D^\\ast(x|\\theta_{D^\\ast})) &= p_{\\text{data}}(x) \\\\\n",
    "D^\\ast(x|\\theta_{D^\\ast}) &= \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)} \n",
    "\\end{align*}\n",
    "\n",
    "We have found the optimal discriminator given a generator. Let us focus now on the generator's objective function which is essentially to minimize the discriminator's objective function.\n",
    "\n",
    "\\begin{align*}\n",
    "J(G(x|\\theta_{G})) &= \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{D^\\ast(x|\\theta_{D^\\ast})} + \\mathbb{E}_{x \\sim p_{G}(x)} \\log{(1 - D^\\ast(x|\\theta_{D^\\ast}))} \\\\\n",
    "&= \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{\\bigg( \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}} \\bigg) + \\mathbb{E}_{x \\sim p_{G}(x)} \\log{\\bigg(1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}\\bigg)} \\\\\n",
    "&= \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{\\bigg( \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}} \\bigg) + \\mathbb{E}_{x \\sim p_{G}(x)} \\log{\\bigg(\\frac{p_{G}(x)}{p_{\\text{data}}(x) + p_{G}(x)}\\bigg)} \\\\\n",
    "&= \\int_{x} p_{\\text{data}}(x) \\log{\\bigg( \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}} \\bigg) dx + \\int_{x} p_{G}(x) \\log{\\bigg(\\frac{p_{G}(x)}{p_{\\text{data}}(x) + p_{G}(x)}\\bigg)} dx\n",
    "\\end{align*}\n",
    "\n",
    "We will note the Kullback–Leibler (KL) divergences in the above objective function for the generator.\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\int_{x} p(x) \\log\\bigg(\\frac{p(x)}{q(x)}\\bigg) dx\n",
    "$$\n",
    "\n",
    "Recall the definition of a $\\lambda$ divergence.\n",
    "\n",
    "$$\n",
    "D_{\\lambda}(P||Q) = \\lambda D_{KL}(P||\\lambda P + (1 - \\lambda) Q) + (1 - \\lambda) D_{KL}(Q||\\lambda P + (1 - \\lambda) Q)\n",
    "$$\n",
    "\n",
    "If $\\lambda$ takes the value of 0.5 this is then called the Jensen-Shannon (JS) divergence. This divergence is symmetric and non-negative.\n",
    "\n",
    "$$\n",
    "D_{JS}(P||Q) = 0.5 D_{KL}\\bigg(P\\bigg|\\bigg|\\frac{P + Q}{2}\\bigg) + 0.5 D_{KL}\\bigg(Q\\bigg|\\bigg|\\frac{P + Q}{2}\\bigg)\n",
    "$$\n",
    "\n",
    "Keeping this in mind let us take a look again at the objective function of the generator.\n",
    "\n",
    "\\begin{align*}\n",
    "J(G(x|\\theta_{G})) &= \\int_{x} p_{\\text{data}}(x) \\log{\\bigg( \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}} \\bigg) dx + \\int_{x} p_{G}(x) \\log{\\bigg(\\frac{p_{G}(x)}{p_{\\text{data}}(x) + p_{G}(x)}\\bigg)} dx \\\\\n",
    "&= \\int_{x} p_{\\text{data}}(x) \\log{\\bigg(\\frac{2}{2}\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}} \\bigg) dx + \\int_{x} p_{G}(x) \\log{\\bigg(\\frac{2}{2}\\frac{p_{G}(x)}{p_{\\text{data}}(x) + p_{G}(x)}\\bigg)} dx \\\\\n",
    "&= \\int_{x} p_{\\text{data}}(x) \\log{\\bigg(\\frac{1}{2}\\frac{1}{0.5}\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{G}(x)}} \\bigg) dx + \\int_{x} p_{G}(x) \\log{\\bigg(\\frac{1}{2}\\frac{1}{0.5}\\frac{p_{G}(x)}{p_{\\text{data}}(x) + p_{G}(x)}\\bigg)} dx \\\\\n",
    "&= \\int_{x} p_{\\text{data}}(x) \\bigg[ \\log(0.5) + \\log{\\bigg(\\frac{p_{\\text{data}}(x)}{0.5 (p_{\\text{data}}(x) + p_{G}(x))}} \\bigg) \\bigg] dx \\\\ &+ \\int_{x} p_{G}(x) \\bigg[\\log(0.5) + \\log{\\bigg(\\frac{p_{G}(x)}{0.5 (p_{\\text{data}}(x) + p_{G}(x))}\\bigg) \\bigg] } dx \\\\\n",
    "&= \\log\\bigg(\\frac{1}{4}\\bigg) + \\int_{x} p_{\\text{data}}(x) \\bigg[\\log{\\bigg(\\frac{p_{\\text{data}}(x)}{0.5 (p_{\\text{data}}(x) + p_{G}(x))}} \\bigg) \\bigg] dx \\\\ \n",
    "&+ \\int_{x} p_{G}(x) \\bigg[\\log{\\bigg(\\frac{p_{G}(x)}{0.5 (p_{\\text{data}}(x) + p_{G}(x))}\\bigg) \\bigg] } dx \\\\\n",
    "&= -\\log(4) + D_{KL}\\bigg(P_{\\text{data}}\\bigg|\\bigg|\\frac{P_{\\text{data}} + P_{G}}{2}\\bigg) + D_{KL}\\bigg(P_{G}\\bigg|\\bigg|\\frac{P_{\\text{data}} + P_{G}}{2}\\bigg) \\\\\n",
    "&= -\\log(4) + 2 \\bigg(0.5 D_{KL}\\bigg(P_{\\text{data}}\\bigg|\\bigg|\\frac{P_{\\text{data}} + P_{G}}{2}\\bigg) + 0.5 D_{KL}\\bigg(P_{G}\\bigg|\\bigg|\\frac{P_{\\text{data}} + P_{G}}{2}\\bigg)\\bigg) \\\\\n",
    "&= -\\log(4) + 2D_{JS}(P_{\\text{data}}||P_{G}) \n",
    "\\end{align*}\n",
    "\n",
    "It is clear from the objective function of the generator above that the global minimum value attained is $-\\log(4)$ which occurs when the following holds.\n",
    "\n",
    "$$\n",
    "P_{G}=P_{\\text{data}}\n",
    "$$\n",
    "\n",
    "When the above holds the Jensen-Shannon divergence, i.e., $D_{JS}(P_{\\text{data}}||P_{G})$, will be zero. Hence we have shown that the optimal solution is as follows.\n",
    "\n",
    "$$\n",
    "P_{G}=P_{\\text{data}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game: Convergence\n",
    "\n",
    "---\n",
    "\n",
    "Assuming that the discriminator is allowed to reach its optimum given a generator, then $P_{G}$ can be shown to converge to $P_{\\text{data}}$. \n",
    "\n",
    "Consider the following objective function which has been previously shown to be convex with respect to $P_{G}$ as we found the global minimum at $-\\log(4)$.\n",
    "\n",
    "$$\n",
    "U(D^\\ast, P_{G}) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\log{D^\\ast(x|\\theta_{D^\\ast})} + \\mathbb{E}_{x \\sim p_{G}(x)} \\log{(1 - D^\\ast(x|\\theta_{D^\\ast}))}\n",
    "$$\n",
    "\n",
    "Gradient descent is used by the generator to move towards the global minimum given an optimal discriminator. We will show that the gradient of the generator exists given an optimal discriminator, i.e., $\\nabla_{P_{G}} U(D^\\ast, P_{G})$, such that convergence of $P_{G}$ to $P_{\\text{data}}$ is guaranteed.\n",
    "\n",
    "Note that the following is a supremum of a set of convex functions where the set is indexed by the discriminator $D$: $U(D^\\ast, P_{G})=\\sup_{D} U(D, P_{G})$. Remember that the supremum is the least upper bound.\n",
    "\n",
    "Let us recall a few definitions regarding gradients and subgradients. A vector $g \\in \\mathbb{R}^K$ is a subgradient of a function $f: \\mathbb{R}^K \\rightarrow \\mathbb{R}$ at a point $x \\in \\mathbb{dom}(f)$ if $\\forall z \\in \\mathbb{dom}(f)$, the following relationship holds:\n",
    "\n",
    "$$\n",
    "f(z) \\geq f(x) + g^{T}(z - x)\n",
    "$$\n",
    "\n",
    "If $f$ is convex and differentiable then its gradient at a point $x$ is also the subgradient. Most importantly, a subgradient can exist even if $f$ is not differentiable.\n",
    "\n",
    "The subgradients of the supremum of a set of convex functions include the subgradient of the function at the point where the supremum is attained. As mentioned earlier, we have already shown that $U(D^\\ast, P_{G})$ is convex.\n",
    "\n",
    "\\begin{align*}\n",
    "&U(D^\\ast, P_{G})=\\sup_{D} U(D, P_{G}) \\\\\n",
    "&\\nabla_{P_{G}} \\sup_{D} U(D, P_{G}) \\in \\nabla_{P_{G}} U(D, P_{G}) \\\\\n",
    "&\\nabla_{P_{G}} U(D^\\ast, P_{G}) \\in \\nabla_{P_{G}} U(D, P_{G})\n",
    "\\end{align*}\n",
    "\n",
    "The gradient of the generator, $\\nabla_{P_{G}} U(D^\\ast, P_{G})$, is used to make incremental improvements to the objective function of the generator, $U(D^\\ast, P_{G})$, given an optimal discriminator, $D^\\ast$. Therefore convergence of $P_{G}$ to $P_{\\text{data}}$ is guaranteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeAdversarialNetworkDiscriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(frame=inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "        self.feature_extractor = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64,\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding=\"same\",\n",
    "                    strides=(2, 2),\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                    activation=\"tanh\",\n",
    "                ),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64,\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding=\"same\",\n",
    "                    strides=(2, 2),\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                    activation=\"tanh\",\n",
    "                ),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                tf.keras.layers.Flatten(data_format=\"channels_last\"),\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=1024,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                    activation=\"tanh\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.discriminator = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=1,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                    activation=\"linear\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        encoding = self.feature_extractor(x)\n",
    "        return self.discriminator(encoding)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "\n",
    "class GenerativeAdversarialNetworkGenerator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(frame=inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "        self.generator = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=1024,\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=128 * 7 * 7,\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 128)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64,\n",
    "                    kernel_size=(5, 5),\n",
    "                    strides=(2, 2),\n",
    "                    padding=\"same\",\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64,\n",
    "                    kernel_size=(5, 5),\n",
    "                    strides=(1, 1),\n",
    "                    padding=\"same\",\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                ),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64,\n",
    "                    kernel_size=(5, 5),\n",
    "                    strides=(2, 2),\n",
    "                    padding=\"same\",\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=1,\n",
    "                    kernel_size=(5, 5),\n",
    "                    strides=(1, 1),\n",
    "                    padding=\"same\",\n",
    "                    activation=\"sigmoid\",\n",
    "                    kernel_initializer=\"glorot_normal\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-8),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 100\n",
    "z_size = 2\n",
    "generator = GenerativeAdversarialNetworkGenerator()\n",
    "discriminator = GenerativeAdversarialNetworkDiscriminator()\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        tensors=(\n",
    "            x_train.reshape(\n",
    "                x_train.shape[0],\n",
    "                x_train.shape[1],\n",
    "                x_train.shape[2],\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size=batch_size)\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    tensors=(\n",
    "        x_test.reshape(\n",
    "            x_test.shape[0],\n",
    "            x_test.shape[1],\n",
    "            x_test.shape[2],\n",
    "            1,\n",
    "        )\n",
    "    )\n",
    ").batch(batch_size=batch_size)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4, amsgrad=True, clipvalue=1.0\n",
    ")\n",
    "optimizer_gen = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4, amsgrad=True, clipvalue=1.0\n",
    ")\n",
    "train_disc_loss = tf.keras.metrics.Mean(name=\"train_disc_loss\")\n",
    "train_gen_loss = tf.keras.metrics.Mean(name=\"train_gen_loss\")\n",
    "test_disc_loss = tf.keras.metrics.Mean(name=\"test_disc_loss\")\n",
    "test_gen_loss = tf.keras.metrics.Mean(name=\"test_gen_loss\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, truth_disc, z, truth_gen):\n",
    "    with tf.GradientTape() as tape:\n",
    "        disc_preds = discriminator(x=x, training=True)\n",
    "        disc_loss = reconstruction_loss(y_true=truth_disc, y_pred=disc_preds)\n",
    "    gradients_disc = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    optimizer_disc.apply_gradients(\n",
    "        grads_and_vars=zip(\n",
    "            gradients_disc,\n",
    "            discriminator.trainable_variables,\n",
    "        )\n",
    "    )\n",
    "    train_disc_loss(disc_loss)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        gen_preds = generator(z=z, training=True)\n",
    "        gen_preds = discriminator(x=gen_preds, training=True)\n",
    "        gen_loss = reconstruction_loss(y_true=truth_gen, y_pred=gen_preds)\n",
    "    gradients_gen = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    optimizer_gen.apply_gradients(\n",
    "        grads_and_vars=zip(\n",
    "            gradients_gen,\n",
    "            generator.trainable_variables,\n",
    "        )\n",
    "    )\n",
    "    train_gen_loss(gen_loss)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, truth_disc, z, truth_gen):\n",
    "    disc_preds = discriminator(x=x, training=False)\n",
    "    disc_loss = reconstruction_loss(y_true=truth_disc, y_pred=disc_preds)\n",
    "    test_disc_loss(disc_loss)\n",
    "\n",
    "    gen_preds = generator(z=z, training=False)\n",
    "    gen_preds = discriminator(x=gen_preds, training=False)\n",
    "    gen_loss = reconstruction_loss(y_true=truth_gen, y_pred=gen_preds)\n",
    "    test_gen_loss(gen_loss)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_disc_loss.reset_states()\n",
    "    train_gen_loss.reset_states()\n",
    "    test_disc_loss.reset_states()\n",
    "    test_gen_loss.reset_states()\n",
    "\n",
    "    for real_data in train_ds:\n",
    "        train_step(\n",
    "            x=np.concatenate(\n",
    "                (\n",
    "                    real_data,\n",
    "                    generator(\n",
    "                        z=np.random.uniform(\n",
    "                            low=-1.0, high=1.0, size=(batch_size, z_size)\n",
    "                        ),\n",
    "                        training=True,\n",
    "                    ),\n",
    "                ),\n",
    "                axis=0,\n",
    "            ),\n",
    "            truth_disc=np.concatenate(\n",
    "                (np.ones(shape=(batch_size, 1)), np.zeros(shape=(batch_size, 1))),\n",
    "                axis=0,\n",
    "            )\n",
    "            + (0.05 * np.random.random(size=(batch_size * 2, 1))),\n",
    "            z=np.random.uniform(low=-1.0, high=1.0, size=(batch_size, z_size)),\n",
    "            truth_gen=np.ones(shape=(batch_size, 1)),\n",
    "        )\n",
    "\n",
    "    for real_data in test_ds:\n",
    "        test_step(\n",
    "            x=np.concatenate(\n",
    "                (\n",
    "                    real_data,\n",
    "                    generator(\n",
    "                        z=np.random.uniform(\n",
    "                            low=-1.0, high=1.0, size=(batch_size, z_size)\n",
    "                        ),\n",
    "                        training=False,\n",
    "                    ),\n",
    "                ),\n",
    "                axis=0,\n",
    "            ),\n",
    "            truth_disc=np.concatenate(\n",
    "                (np.ones(shape=(batch_size, 1)), np.zeros(shape=(batch_size, 1))),\n",
    "                axis=0,\n",
    "            )\n",
    "            + (0.05 * np.random.random(size=(batch_size * 2, 1))),\n",
    "            z=np.random.uniform(low=-1.0, high=1.0, size=(batch_size, z_size)),\n",
    "            truth_gen=np.ones(shape=(batch_size, 1)),\n",
    "        )\n",
    "\n",
    "    img = tf.keras.preprocessing.image.array_to_img(\n",
    "        x=generator(\n",
    "            z=np.random.uniform(low=-1.0, high=1.0, size=(1, z_size)),\n",
    "            training=False,\n",
    "        )[0, :, :, :]\n",
    "        * 255.0,\n",
    "        scale=False,\n",
    "    )\n",
    "    img.save(fp=\"/home/hamaad/Projects/gan_tensorflow/generated_image.png\")\n",
    "    pil_im = Image.open(\"/home/hamaad/Projects/gan_tensorflow/generated_image.png\", \"r\")\n",
    "    plt.imshow(X=np.asarray(a=pil_im), cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"Train discriminator loss:\", train_disc_loss.result())\n",
    "    print(\"Train generator loss:\", train_gen_loss.result())\n",
    "    print(\"Test discriminator loss:\", test_disc_loss.result())\n",
    "    print(\"Test generator loss:\", test_gen_loss.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dcgan = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", MinMaxScaler(feature_range=(0.0, 1.0))),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            linear_model.LogisticRegression(max_iter=10000, random_state=666),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with tf.device(device_name=\"/CPU:0\"):\n",
    "    pipe_dcgan.fit(\n",
    "        X=discriminator.encoder(\n",
    "            sampled_x_train.reshape(\n",
    "                sampled_x_train.shape[0],\n",
    "                sampled_x_train.shape[1],\n",
    "                sampled_x_train.shape[2],\n",
    "                1,\n",
    "            )\n",
    "        ),\n",
    "        y=sampled_y_train,\n",
    "    )\n",
    "    acc_dcgan = pipe_dcgan.score(\n",
    "        X=discriminator.encoder(\n",
    "            x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "        ),\n",
    "        y=y_test,\n",
    "    )\n",
    "\n",
    "print(\n",
    "    \"The accuracy score for the MNIST classification task with DCGAN: %.6f%%.\"\n",
    "    % (acc_dcgan * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "data_size = 28\n",
    "figure = np.zeros(shape=(data_size * n, data_size * n))\n",
    "grid_x = np.linspace(start=-1.0, stop=1.0, num=n)\n",
    "grid_y = np.linspace(start=-1.0, stop=1.0, num=n)\n",
    "\n",
    "with tf.device(device_name=\"/CPU:0\"):\n",
    "    for i, xi in enumerate(grid_x):\n",
    "        for j, yi in enumerate(grid_y):\n",
    "            figure[\n",
    "                i * data_size : (i + 1) * data_size,\n",
    "                j * data_size : (j + 1) * data_size,\n",
    "            ] = (\n",
    "                generator(np.array(object=[[xi, yi]]))\n",
    "                .numpy()\n",
    "                .reshape(data_size, data_size)\n",
    "            )\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(X=figure, cmap=\"Greys_r\")\n",
    "plt.title(\n",
    "    label=\"Deep Convolutional Generative Adversarial Network (DCGAN) with a 2-dimensional latent manifold\\nGenerating new images on the 2-dimensional latent manifold\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.xlabel(xlabel=\"Latent dimension 1\", fontsize=24)\n",
    "plt.ylabel(ylabel=\"Latent dimension 2\", fontsize=24)\n",
    "plt.savefig(fname=\"/home/hamaad/Projects/gan_tensorflow/DCGAN_Generated_Images.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "---\n",
    "\n",
    "In these experiments we show the ability of the generator to create realistic synthetic examples for the MNIST dataset. \n",
    "\n",
    "Finally we show that using the representation learnt by the discriminator we can attain competitive results to using other representation learning methods for the MNIST dataset such as a wide variety of autoencoders.\n",
    "\n",
    "### Results: Generating new data\n",
    "\n",
    "---\n",
    "\n",
    "![](DCGAN_Generated_Images.png)\n",
    "\n",
    "\n",
    "### Results: GAN for representation learning\n",
    "\n",
    "---\n",
    "\n",
    "* The accuracy score for the MNIST classification task with DCGAN: 94.01%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "We have shown how to use GANs to learn a good representation of raw data, i.e., 1 or 2 dimensional tensors per unit of analysis, that can then perhaps be used for supervised learning tasks in the domain of computer vision. This moves us away from manual handcrafted feature engineering towards automatic feature engineering, i.e., representation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y. and Courville A. (2016). Deep Learning (MIT Press).\n",
    "2. Geron, A. (2017). Hands-On Machine Learning with Scikit-Learn & Tensorflow (O'Reilly).\n",
    "3. Radford, A., Luke, M. and Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (https://arxiv.org/abs/1511.06434).\n",
    "4. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y. (2014). Generative Adversarial Networks (https://arxiv.org/abs/1406.2661).\n",
    "5. http://scikit-learn.org/stable/#\n",
    "6. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "7. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n",
    "8. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "9. https://keras.io\n",
    "10. https://github.com/fchollet/keras/blob/master/examples/mnist_acgan.py#L24\n",
    "11. https://en.wikipedia.org/wiki/Kullback–Leibler_divergence\n",
    "12. https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
